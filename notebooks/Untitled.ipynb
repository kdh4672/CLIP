{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58edbdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m text_descriptions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a photo of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m testset\u001b[38;5;241m.\u001b[39mclasses]\n\u001b[1;32m     58\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize(text_descriptions)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 59\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (b,77)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m text_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtoken_embedding(text)  \u001b[38;5;66;03m# shape: (b,77,512)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m prev_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/DCSS/Codes/CLIP/clip/clip.py:219\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    217\u001b[0m sot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|startoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    218\u001b[0m eot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[sot_token] \u001b[38;5;241m+\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencode(text) \u001b[38;5;241m+\u001b[39m [eot_token] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m    220\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n",
      "File \u001b[0;32m~/DCSS/Codes/CLIP/clip/clip.py:219\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    217\u001b[0m sot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|startoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    218\u001b[0m eot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[sot_token] \u001b[38;5;241m+\u001b[39m \u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [eot_token] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m    220\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n",
      "File \u001b[0;32m~/DCSS/Codes/CLIP/clip/simple_tokenizer.py:123\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    122\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 123\u001b[0m     text \u001b[38;5;241m=\u001b[39m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[1;32m    125\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/DCSS/Codes/CLIP/clip/simple_tokenizer.py:51\u001b[0m, in \u001b[0;36mbasic_clean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[0;32m---> 51\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/kcc/lib/python3.8/site-packages/ftfy/__init__.py:303\u001b[0m, in \u001b[0;36mfix_text\u001b[0;34m(text, config, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[0;32m--> 303\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "from re import L\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import kclip.model\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import pytorch_model_summary\n",
    "from Metrics import nmi, acc, Kmeans_model_evaluation, Kmeans_model_evaluation_text_representation\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from adamp import AdamP\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-5,\n",
    "#                     help=\"no\")\n",
    "# parser.add_argument(\"--log_dir\", type=str,  default='runs/test',\n",
    "#                     help=\"no\")\n",
    "# parser.add_argument(\"--total_epoch\", type=int, default=20,\n",
    "#                     help=\"no\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=20,\n",
    "#                     help=\"no\")\n",
    "# args = parser.parse_args()\n",
    "# lr = args.lr\n",
    "# log_dir = args.log_dir\n",
    "# total_epoch = args.total_epoch\n",
    "# batch_size = args.batch_size\n",
    "# dict_ = vars(args)\n",
    "lr = 1e-5\n",
    "log_dir = 'runs/test'\n",
    "total_epoch = 20\n",
    "batch_size = 20\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "print('Device:', device)\n",
    "\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "model.eval()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=preprocess)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in testset.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions).cuda()\n",
    "text_embedding = model.token_embedding(text)  # shape: (b,77,512)\n",
    "\n",
    "prev_acc = 0\n",
    "for epoch in range(total_epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        text_features = model.encode_text(text_embedding)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
